{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Image Grading - Kaggle Training Notebook\n",
    "\n",
    "This notebook trains the image quality scoring model on Kaggle using the Adobe FiveK dataset.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Upload this notebook to Kaggle\n",
    "2. Add the Adobe FiveK dataset to the notebook\n",
    "3. Enable GPU accelerator (Settings > Accelerator > GPU)\n",
    "4. Run all cells\n",
    "5. Download the trained model from the output section\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- Adobe FiveK: https://data.csail.mit.edu/graphics/fivek/\n",
    "- Or search for \"Adobe FiveK\" on Kaggle Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install additional dependencies if needed\n# Most packages (numpy, opencv, tensorflow) are pre-installed on Kaggle\n!pip install imageio -q"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Project Files\n",
    "\n",
    "Upload the `src/` directory from this project, or clone from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Clone from GitHub (replace with your repo URL)\n",
    "# !git clone https://github.com/yourusername/ML-Image_grading.git\n",
    "# import sys\n",
    "# sys.path.append('/kaggle/working/ML-Image_grading/src')\n",
    "\n",
    "# Option 2: Upload files to Kaggle Dataset and add it to the notebook\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/ml-image-grading/src')  # Adjust path as needed\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Import project modules\n",
    "from image_loader import CR2ImageLoader\n",
    "from feature_extractor import ImageFeatureExtractor\n",
    "from scoring_model import ImageScoringModel\n",
    "from train_model import ModelTrainer\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Dataset Path\n",
    "\n",
    "Update this path to point to your Adobe FiveK dataset location in Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this path based on where you uploaded/added the Adobe FiveK dataset\n",
    "DATASET_PATH = '/kaggle/input/adobe-fivek'  # Update this path\n",
    "OUTPUT_PATH = '/kaggle/working/models'\n",
    "MODEL_NAME = 'image_quality_scorer.h5'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "image_loader = CR2ImageLoader(target_size=(512, 512))\n",
    "feature_extractor = ImageFeatureExtractor()\n",
    "model = ImageScoringModel(feature_dim=30, input_shape=(512, 512, 3))\n",
    "\n",
    "# Create trainer\n",
    "trainer = ModelTrainer(model, feature_extractor, image_loader)\n",
    "\n",
    "print(\"✓ Components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EXPERT = 'c'  # Expert C is most commonly used\n",
    "NUM_SAMPLES = 1000  # Use None for all samples (takes longer)\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Expert: {EXPERT}\")\n",
    "print(f\"  Samples: {NUM_SAMPLES if NUM_SAMPLES else 'All'}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = trainer.train_on_adobe_fivek(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    expert=EXPERT,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = [\n",
    "    ('overall_score_loss', 'Overall Score Loss'),\n",
    "    ('composition_score_loss', 'Composition Score Loss'),\n",
    "    ('color_score_loss', 'Color Score Loss'),\n",
    "    ('technical_score_loss', 'Technical Score Loss')\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    if metric in history:\n",
    "        ax.plot(history[metric], label='Training')\n",
    "        if f'val_{metric}' in history:\n",
    "            ax.plot(history[f'val_{metric}'], label='Validation')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training history plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = os.path.join(OUTPUT_PATH, MODEL_NAME)\n",
    "model.save_model(model_path)\n",
    "\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "print(f\"  File size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "history_path = os.path.join(OUTPUT_PATH, 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    history_serializable = {k: [float(v) for v in vals] for k, vals in history.items()}\n",
    "    json.dump(history_serializable, f, indent=2)\n",
    "\n",
    "print(f\"✓ Training history saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 70)\n",
    "print(model.get_model_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Instructions\n",
    "\n",
    "**To use this model on your Mac:**\n",
    "\n",
    "1. Download the trained model file from the output section (right side panel)\n",
    "2. Look for: `models/image_quality_scorer.h5`\n",
    "3. Place it in your local `models/` directory\n",
    "4. Run the pipeline with: `python src/pipeline.py <image.CR2> --model models/image_quality_scorer.h5`\n",
    "\n",
    "The model is now ready to use locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample image\n",
    "from dataset_loader import AdobeFiveKLoader\n",
    "\n",
    "# Load a test image\n",
    "loader = AdobeFiveKLoader(DATASET_PATH, expert=EXPERT)\n",
    "test_pairs = loader.load_image_pairs(limit=1)\n",
    "\n",
    "if test_pairs:\n",
    "    original, edited, filename = test_pairs[0]\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor.get_feature_vector(original)\n",
    "    \n",
    "    # Get scores\n",
    "    scores = model.predict_score(original, features)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Image: {filename}\")\n",
    "    print(\"\\nPredicted Scores:\")\n",
    "    for key, value in scores.items():\n",
    "        print(f\"  {key}: {value * 100:.2f}/100\")\n",
    "    \n",
    "    # Show image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(edited)\n",
    "    plt.title('Expert Edit')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test images available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}